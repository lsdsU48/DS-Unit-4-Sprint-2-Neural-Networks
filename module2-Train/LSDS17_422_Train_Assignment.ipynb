{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NGGrt9EYlCqY"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Train Practice\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 2*\n",
    "\n",
    "Continue to use TensorFlow Keras & a sample of the [Quickdraw dataset](https://github.com/googlecreativelab/quickdraw-dataset) to build a sketch classification model. The dataset has been sampled to only 10 classes and 10000 observations per class. Please build a baseline classification model then run a few experiments with different optimizers and learning rates. \n",
    "\n",
    "*Don't forgot to switch to GPU on Colab!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ptJ2b3wk62Ud"
   },
   "source": [
    "### Write a function to load your data\n",
    "\n",
    "Wrap yesterday's preprocessing steps into a function that returns four items:\n",
    "* X_train\n",
    "* y_train\n",
    "* X_test\n",
    "* y_test\n",
    "\n",
    "Your function should accept a `path` to the data as a argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 784)\n",
      "(100000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "\n",
    "data = np.load('C:/MyLearning/23-LSDS/04-LSDS17-U4/U4S2-NN/data/quickdraw10.npz')\n",
    "X = data['arr_0']\n",
    "y = data['arr_1']\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size=0.2, random_state=42, stratify = y )\n",
    "\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train / 255.\n",
    "X_test = X_test / 255.\n",
    "\n",
    "X_train = X_train.reshape((60000, 784))\n",
    "X_test = X_test.reshape((10000, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nJsIsrvp7O3e"
   },
   "outputs": [],
   "source": [
    "def load_quickdraw10(data):\n",
    "\n",
    "  return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l-6PxI6H5__2"
   },
   "source": [
    "### Write a Model Function\n",
    "Using your model from yesterday, write a function called `create_model` which returns a compiled TensorFlow Keras Sequential Model suitable for classifying the QuickDraw-10 dataset. Include parameters for the following: \n",
    "* Learning Rate\n",
    "* Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nEREYT-3wI1f"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
    "\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "def create_model(lr=.01):\n",
    "    opt = SGD(learning_rate = lr)\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(32, activation = 'relu', input_dim=784), # 32 as basic/default for INPUT\n",
    "        Dense(32, activation = 'relu'),             # hidden layer?\n",
    "        Dense(10, activation = 'softmax')           # in the output layer. 10 neuron (because of 10 classes) on the output layer\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', \n",
    "                         metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN architectur\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "\n",
    "  def __init__(self):\n",
    "    self.W = tf.Variable(8.0)\n",
    "    self.b = tf.Variable(40.0)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    return self.W * x + self.b\n",
    "# If we want to change from regression to classification, we need to add to RETURN: an activation function\n",
    "\n",
    "\n",
    "model = Model()\n",
    "\n",
    "assert model(3.0).numpy() == 64.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(3.0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "\n",
    "def loss(target_y, predicted_y):\n",
    "  \"MSE\"\n",
    "  return tf.reduce_mean(tf.square(target_y - predicted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update weights\n",
    "\n",
    "def train(model, inputs, outputs, learning_rate):\n",
    "          \n",
    "\n",
    "   with tf.GradientTape() as t:  # this is to record the order & all happenings for each observation(loss, etc)\n",
    "       current_loss = loss(outputs, model(inputs))\n",
    "    \n",
    "   # partial derivative of WEIGHT & BIAS\n",
    "   dW, db = t.gradient(current_loss, [model.W, model.b]) ## loss based on bias & weight (d = derivative)\n",
    "   model.W.assign_sub(learning_rate * dW) # UPdate the weight.\n",
    "   model.b.assign_sub(learning_rate * db) # update the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f0pCkh8C7eGL"
   },
   "source": [
    "### Experiment with Batch Size\n",
    "* Run 5 experiments with various batch sizes of your choice. \n",
    "* Visualize the results\n",
    "* Write up an analysis of the experiments and select the \"best\" performing model among your experiments. Make sure to compare against your model's performance yesterday. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USXjs7Hk71Hy"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-4c51075c3b7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m   \u001b[0mWs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m   \u001b[0mbs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m   \u001b[0mcurrent_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m   \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'outputs' is not defined"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "\n",
    "# Store Some history of weights\n",
    "Ws, bs = [], []\n",
    "epochs = range(20)\n",
    "for epoch in epochs:\n",
    "  Ws.append(model.W.numpy())\n",
    "  bs.append(model.b.numpy())\n",
    "  current_loss = loss(outputs, model(inputs))\n",
    "\n",
    "  train(model, inputs, outputs, learning_rate=0.1)\n",
    "  print('Epoch %2d: W=%1.2f b=%1.2f loss=%2.5f' % (epoch, Ws[-1], bs[-1], current_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8b-r70o8p2Dm"
   },
   "source": [
    "### Experiment with Learning Rate\n",
    "* Run 5 experiments with various learning rate magnitudes: 1, .1, .01, .001, .0001.\n",
    "* Use the \"best\" batch size from the previous experiment\n",
    "* Visualize the results\n",
    "* Write up an analysis of the experiments and select the \"best\" performing model among your experiments. Make sure to compare against the previous experiments and your model's performance yesterday. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_SA144xx8Luf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gxMtSRhV9Q7I"
   },
   "source": [
    "### Experiment with different Optimizers\n",
    "* Run 5 experiments with various optimizers available in TensorFlow. See list [here](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)\n",
    "* Visualize the results\n",
    "* Write up an analysis of the experiments and select the \"best\" performing model among your experiments. Make sure to compare against the previous experiments and your model's performance yesterday.\n",
    "* Repeat the experiment combining Learning Rate and different optimizers. Does the best performing model change? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ujLuzdNA91ip"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ydAqeY9S8uHA"
   },
   "source": [
    "### Additional Written Tasks\n",
    "\n",
    "1. Describe the process of backpropagation in your own words: \n",
    "```\n",
    "Your answer goes here.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FwlRJSfBlCvy"
   },
   "source": [
    "## Stretch Goals: \n",
    "\n",
    "- Implement GridSearch on anyone of the experiments\n",
    "- On the learning rate experiments, implement [EarlyStopping](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping)\n",
    "- Review material on the math behind gradient descent: \n",
    "\n",
    "  - Gradient Descent\n",
    "    - Gradient Descent, Step-by-Step  by StatQuest w/ Josh Starmer. This will help you understand the gradient descent based optimization that happens underneath the hood of neural networks. It uses a non-neural network example, which I believe is a gentler introduction. You will hear me refer to this technique as \"vanilla\" gradient descent. \n",
    "    - Stochastic Gradient Descent, Clearly Explained!!! by StatQuest w/ Josh Starmer. This builds on the techniques in the previous video.  This technique is the one that is actually implemented inside modern 'nets. \n",
    "These are great resources to help you understand tomorrow's material at a deeper level. I highly recommend watching these ahead of tomorrow.\n",
    "\n",
    "  - Background Math\n",
    "    - Dot products and duality by 3Blue1Brown. Explains the core linear algebra operation happening in today's perceptron.\n",
    "The paradox of the derivative by 3Blue1Brown. Does a great job explaining a derivative. \n",
    "    - Visualizing the chain rule and product rule by 3Blue1Brown. Explains the black magic that happens within Stochastic Gradient Descent. \n",
    "These math resources are very much optional. They can be very heady, but I encourage you to explore. Your understanding of neural networks will greatly increase if you understand this math background.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_432_Train_Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nteract": {
   "version": "0.22.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
