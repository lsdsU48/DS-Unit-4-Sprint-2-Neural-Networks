{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSDS_422_Train_Assignment-v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "nteract": {
      "version": "0.22.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NGGrt9EYlCqY"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "# Train Practice\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2 Assignment 2*\n",
        "\n",
        "Continue to use TensorFlow Keras & a sample of the [Quickdraw dataset](https://github.com/googlecreativelab/quickdraw-dataset) to build a sketch classification model. The dataset has been sampled to only 10 classes and 10000 observations per class. Please build a baseline classification model then run a few experiments with different optimizers and learning rates. \n",
        "\n",
        "*Don't forgot to switch to GPU on Colab!*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uixmf_vndQdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# learn also from:\n",
        "# https://towardsdatascience.com/writing-your-first-neural-net-in-less-than-30-lines-of-code-with-keras-18e160a35502"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76TuNQwXdQf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivWTX8efdG7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptJ2b3wk62Ud",
        "colab_type": "text"
      },
      "source": [
        "### Write a function to load your data\n",
        "\n",
        "Wrap yesterday's preprocessing steps into a function that returns four items:\n",
        "* X_train\n",
        "* y_train\n",
        "* X_test\n",
        "* y_test\n",
        "\n",
        "Your function should accept a `path` to the data as a argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ic2_dfgAdGaA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJsIsrvp7O3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_quickdraw10(path):\n",
        "\n",
        "  return X_train, y_train, X_test, y_test"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-6PxI6H5__2",
        "colab_type": "text"
      },
      "source": [
        "### Write a Model Function\n",
        "Using your model from yesterday, write a function called `create_model` which returns a compiled TensorFlow Keras Sequential Model suitable for classifying the QuickDraw-10 dataset. Include parameters for the following: \n",
        "* Learning Rate\n",
        "* Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nEREYT-3wI1f",
        "colab": {}
      },
      "source": [
        "##### Your Code Here #####\n",
        "\n",
        "# def create_model(...):\n",
        "  \n",
        "#   return model"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d49Zm6USd3BT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Build the model\n",
        "\n",
        "# 1a. initiate model\n",
        "network = models.Sequential()\n",
        "\n",
        "\n",
        "# 1b. add the layers (DENSE) to the model: expected input=28*28 (=784), and output=10\n",
        "network.add(layers.Dense(784, activation='relu', input_shape=(28 * 28,)))\n",
        "network.add(layers.Dense(784, activation='relu', input_shape=(28 * 28,)))\n",
        "network.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# 1c. Compile the model with my choice of parameters:\n",
        "network.compile(optimizer='adam',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgPcKAWJd3Oy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The original shape of our input was 60000 images with  pixels  28 x 28. \n",
        "# Now reshape our data and split it between train [50000] images and test [10000] images.\n",
        "\n",
        "X_train = X_train.reshape((60000, 28 * 28))\n",
        "X_train = X_train.astype('float32') / 255\n",
        "X_test = X_test.reshape((10000, 28 * 28))\n",
        "X_test = X_test.astype('float32') / 255\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0T01nmGfuBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encode the data\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTJZGz3EfuE0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "ca2355b6-cda9-438a-a4a2-17d2d8712013"
      },
      "source": [
        "# Fit the data\n",
        "\n",
        "network.fit(X_train, y_train, epochs=5, batch_size=32)\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.1804 - accuracy: 0.9446\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 24s 13ms/step - loss: 0.0808 - accuracy: 0.9752\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 24s 13ms/step - loss: 0.0565 - accuracy: 0.9822\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 24s 13ms/step - loss: 0.0435 - accuracy: 0.9867\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 23s 12ms/step - loss: 0.0364 - accuracy: 0.9890\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7efd71930cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0pCkh8C7eGL",
        "colab_type": "text"
      },
      "source": [
        "### Experiment with Batch Size\n",
        "* Run 5 experiments with various batch sizes of your choice. \n",
        "* Visualize the results\n",
        "* Write up an analysis of the experiments and select the \"best\" performing model among your experiments. Make sure to compare against your model's performance yesterday. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USXjs7Hk71Hy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8b-r70o8p2Dm"
      },
      "source": [
        "### Experiment with Learning Rate\n",
        "* Run 5 experiments with various learning rate magnitudes: 1, .1, .01, .001, .0001.\n",
        "* Use the \"best\" batch size from the previous experiment\n",
        "* Visualize the results\n",
        "* Write up an analysis of the experiments and select the \"best\" performing model among your experiments. Make sure to compare against the previous experiments and your model's performance yesterday. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SA144xx8Luf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxMtSRhV9Q7I",
        "colab_type": "text"
      },
      "source": [
        "### Experiment with different Optimizers\n",
        "* Run 5 experiments with various optimizers available in TensorFlow. See list [here](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)\n",
        "* Visualize the results\n",
        "* Write up an analysis of the experiments and select the \"best\" performing model among your experiments. Make sure to compare against the previous experiments and your model's performance yesterday.\n",
        "* Repeat the experiment combining Learning Rate and different optimizers. Does the best performing model change? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujLuzdNA91ip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydAqeY9S8uHA",
        "colab_type": "text"
      },
      "source": [
        "### Additional Written Tasks\n",
        "\n",
        "1. Describe the process of backpropagation in your own words: \n",
        "```\n",
        "Backpropagation estimates the amount of error for which the weights of a node in the network are responsible.  \n",
        "Instead of updating the weight with the full amount, it is scaled by the learning rate.  \n",
        "* Back propagation is to update the weight and see the change in errors\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FwlRJSfBlCvy"
      },
      "source": [
        "## Stretch Goals: \n",
        "\n",
        "- Implement GridSearch on anyone of the experiments\n",
        "- On the learning rate experiments, implement [EarlyStopping](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping)\n",
        "- Review material on the math behind gradient descent: \n",
        "\n",
        "  - Gradient Descent\n",
        "    - Gradient Descent, Step-by-Step  by StatQuest w/ Josh Starmer. This will help you understand the gradient descent based optimization that happens underneath the hood of neural networks. It uses a non-neural network example, which I believe is a gentler introduction. You will hear me refer to this technique as \"vanilla\" gradient descent. \n",
        "    - Stochastic Gradient Descent, Clearly Explained!!! by StatQuest w/ Josh Starmer. This builds on the techniques in the previous video.  This technique is the one that is actually implemented inside modern 'nets. \n",
        "These are great resources to help you understand tomorrow's material at a deeper level. I highly recommend watching these ahead of tomorrow.\n",
        "\n",
        "  - Background Math\n",
        "    - Dot products and duality by 3Blue1Brown. Explains the core linear algebra operation happening in today's perceptron.\n",
        "The paradox of the derivative by 3Blue1Brown. Does a great job explaining a derivative. \n",
        "    - Visualizing the chain rule and product rule by 3Blue1Brown. Explains the black magic that happens within Stochastic Gradient Descent. \n",
        "These math resources are very much optional. They can be very heady, but I encourage you to explore. Your understanding of neural networks will greatly increase if you understand this math background.\n",
        "\n",
        "\n"
      ]
    }
  ]
}